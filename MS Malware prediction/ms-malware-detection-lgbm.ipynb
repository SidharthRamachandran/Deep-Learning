{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "67f1ef572ea1f522bcdc6a775bf629ee740d87ac"
   },
   "source": [
    "# Microsoft Malware detection\n",
    "\n",
    "In this kernel, I build a LGBM model using only a subset of the training data, in order to fit in memory.\n",
    "\n",
    "## Notebook  Content\n",
    "1. [Utility functions](#0)\n",
    "1. [Loading the data](#1) <br>\n",
    "    2.1 [Get the files and select the variables](#2.1) <br>\n",
    "    2.2 [Define the type of each variable](#2.2)\n",
    "1. [Feature engineering](#2) <br>\n",
    "    3.1 [Frequency encoding](#3.1) <br>\n",
    "    3.2 [Label encoding](#3.2)\n",
    "1. [Training the model](#3)\n",
    "1. [Feature importance](#4)\n",
    "1. [Getting the predictions to a csv file](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3fc9d6b2803d88673e9455315e823b1407f7ff0f"
   },
   "source": [
    "<a id=\"0\"></a> <br>\n",
    "## 1. Utility functions\n",
    "Before starting, we define a utility function that helps managing memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "9a15f134e550358f13e0f3b937e759ed48fd23d9"
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ea005def562ae65202ec9322bec60fd25a1961e1"
   },
   "source": [
    "<a id=\"1\"></a> <br>\n",
    "## 2. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "import gc\n",
    "import time\n",
    "import sys\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from sklearn import metrics\n",
    "# Plotly library\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "init_notebook_mode(connected=True)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5568d0ede96997c2809e883d5c39cc20b1d8c37c"
   },
   "source": [
    "<a id=\"2.1\"></a> <br>\n",
    "### 2.1 Get the files and select the variables\n",
    " we set the types of each fields in the train set in order to reduce the memory usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "b91444fc96ca462398820c2ba54bccd7b4bf4d03"
   },
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "        'MachineIdentifier':                                    'category',\n",
    "        'ProductName':                                          'category',\n",
    "        'EngineVersion':                                        'category',\n",
    "        'AppVersion':                                           'category',\n",
    "        'AvSigVersion':                                         'category',\n",
    "        'IsBeta':                                               'int8',\n",
    "        'RtpStateBitfield':                                     'float16',\n",
    "        'IsSxsPassiveMode':                                     'int8',\n",
    "        'DefaultBrowsersIdentifier':                            'float16',\n",
    "        'AVProductStatesIdentifier':                            'float32',\n",
    "        'AVProductsInstalled':                                  'float16',\n",
    "        'AVProductsEnabled':                                    'float16',\n",
    "        'HasTpm':                                               'int8',\n",
    "        'CountryIdentifier':                                    'int16',\n",
    "        'CityIdentifier':                                       'float32',\n",
    "        'OrganizationIdentifier':                               'float16',\n",
    "        'GeoNameIdentifier':                                    'float16',\n",
    "        'LocaleEnglishNameIdentifier':                          'int8',\n",
    "        'Platform':                                             'category',\n",
    "        'Processor':                                            'category',\n",
    "        'OsVer':                                                'category',\n",
    "        'OsBuild':                                              'int16',\n",
    "        'OsSuite':                                              'int16',\n",
    "        'OsPlatformSubRelease':                                 'category',\n",
    "        'OsBuildLab':                                           'category',\n",
    "        'SkuEdition':                                           'category',\n",
    "        'IsProtected':                                          'float16',\n",
    "        'AutoSampleOptIn':                                      'int8',\n",
    "        'PuaMode':                                              'category',\n",
    "        'SMode':                                                'float16',\n",
    "        'IeVerIdentifier':                                      'float16',\n",
    "        'SmartScreen':                                          'category',\n",
    "        'Firewall':                                             'float16',\n",
    "        'UacLuaenable':                                         'float32',\n",
    "        'Census_MDC2FormFactor':                                'category',\n",
    "        'Census_DeviceFamily':                                  'category',\n",
    "        'Census_OEMNameIdentifier':                             'float16',\n",
    "        'Census_OEMModelIdentifier':                            'float32',\n",
    "        'Census_ProcessorCoreCount':                            'float16',\n",
    "        'Census_ProcessorManufacturerIdentifier':               'float16',\n",
    "        'Census_ProcessorModelIdentifier':                      'float16',\n",
    "        'Census_ProcessorClass':                                'category',\n",
    "        'Census_PrimaryDiskTotalCapacity':                      'float32',\n",
    "        'Census_PrimaryDiskTypeName':                           'category',\n",
    "        'Census_SystemVolumeTotalCapacity':                     'float32',\n",
    "        'Census_HasOpticalDiskDrive':                           'int8',\n",
    "        'Census_TotalPhysicalRAM':                              'float32',\n",
    "        'Census_ChassisTypeName':                               'category',\n",
    "        'Census_InternalPrimaryDiagonalDisplaySizeInInches':    'float16',\n",
    "        'Census_InternalPrimaryDisplayResolutionHorizontal':    'float16',\n",
    "        'Census_InternalPrimaryDisplayResolutionVertical':      'float16',\n",
    "        'Census_PowerPlatformRoleName':                         'category',\n",
    "        'Census_InternalBatteryType':                           'category',\n",
    "        'Census_InternalBatteryNumberOfCharges':                'float32',\n",
    "        'Census_OSVersion':                                     'category',\n",
    "        'Census_OSArchitecture':                                'category',\n",
    "        'Census_OSBranch':                                      'category',\n",
    "        'Census_OSBuildNumber':                                 'int16',\n",
    "        'Census_OSBuildRevision':                               'int32',\n",
    "        'Census_OSEdition':                                     'category',\n",
    "        'Census_OSSkuName':                                     'category',\n",
    "        'Census_OSInstallTypeName':                             'category',\n",
    "        'Census_OSInstallLanguageIdentifier':                   'float16',\n",
    "        'Census_OSUILocaleIdentifier':                          'int16',\n",
    "        'Census_OSWUAutoUpdateOptionsName':                     'category',\n",
    "        'Census_IsPortableOperatingSystem':                     'int8',\n",
    "        'Census_GenuineStateName':                              'category',\n",
    "        'Census_ActivationChannel':                             'category',\n",
    "        'Census_IsFlightingInternal':                           'float16',\n",
    "        'Census_IsFlightsDisabled':                             'float16',\n",
    "        'Census_FlightRing':                                    'category',\n",
    "        'Census_ThresholdOptIn':                                'float16',\n",
    "        'Census_FirmwareManufacturerIdentifier':                'float16',\n",
    "        'Census_FirmwareVersionIdentifier':                     'float32',\n",
    "        'Census_IsSecureBootEnabled':                           'int8',\n",
    "        'Census_IsWIMBootEnabled':                              'float16',\n",
    "        'Census_IsVirtualDevice':                               'float16',\n",
    "        'Census_IsTouchEnabled':                                'int8',\n",
    "        'Census_IsPenCapable':                                  'int8',\n",
    "        'Census_IsAlwaysOnAlwaysConnectedCapable':              'float16',\n",
    "        'Wdft_IsGamer':                                         'float16',\n",
    "        'Wdft_RegionIdentifier':                                'float16',\n",
    "        'HasDetections':                                        'int8'\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ba5573cfaec6625aed13e98c6e034809e2997b5b"
   },
   "source": [
    "First, we make a census of the variables, by type, and define the set we want to keep before reading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "057a508919fad540d226da391cff358b85556b74"
   },
   "outputs": [],
   "source": [
    "numerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "numerical_columns = [c for c,v in dtypes.items() if v in numerics]\n",
    "categorical_columns = [c for c,v in dtypes.items() if v not in numerics]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "23a93bf7ad302d8fe733b9d70de5eed27945979d"
   },
   "source": [
    "We read the data, limiting the size of the training set to 4'000'000 rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#nrows = 9000000\n",
    "nrows =1000000\n",
    "#_______________________________________________________________________________\n",
    "retained_columns = numerical_columns + categorical_columns\n",
    "train = pd.read_csv('../input/microsoft-malware-prediction/train.csv',\n",
    "                    nrows = nrows,\n",
    "                    usecols = retained_columns,\n",
    "                    dtype = dtypes)\n",
    "tqdm.pandas()\n",
    "train.progress_apply(lambda x:x)\n",
    "#_______________________________________________________________\n",
    "retained_columns += ['MachineIdentifier']\n",
    "retained_columns.remove('HasDetections')\n",
    "test = pd.read_csv('../input/microsoft-malware-prediction/test.csv',\n",
    "                   usecols = retained_columns,\n",
    "                   dtype = dtypes,nrows=nrows)\n",
    "tqdm.pandas()\n",
    "test.progress_apply(lambda x:x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualise the amount of missing data in train and test set, we will create new datasets from test,train containing features with NaN values.LightGBM can treat missing values as missing. Still we do it to know about the amount of missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avail(a,b):\n",
    "    train_data = (train.count() / len(train)).drop('HasDetections').sort_values().values\n",
    "    train_data=train_data[a:b]\n",
    "    ind = np.arange(len(train_data))\n",
    "\n",
    "    width = 0.35\n",
    "    fig, axes = plt.subplots(1,1,figsize=(14, 6), dpi=100)\n",
    "    tr = axes.bar(ind, train_data, width, color='royalblue')\n",
    "\n",
    "    test_data = (test.count() / len(test)).sort_values().values\n",
    "    test_data=test_data[a:b]\n",
    "    tt = axes.bar(ind+width, test_data, width, color='seagreen')\n",
    "\n",
    "    axes.set_ylabel('Amount of data available');\n",
    "    axes.set_xticks(ind + width / 2)\n",
    "    axes.tick_params(axis='x')\n",
    "    axes.tick_params(axis='y')\n",
    "    axes.set_xticklabels((train.count() / len(train)).drop('HasDetections')[a:].sort_values().index, rotation=40)\n",
    "    axes.legend([tr, tt], ['Train', 'Test']);\n",
    "    import gc\n",
    "    del train_data\n",
    "    del test_data\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avail(0,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avail(10,19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avail(20,29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avail(30,39)\n",
    "#avail(40,49)\n",
    "#avail(50,59)\n",
    "#avail(60,69)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "123fe175bb92d7262b33649b1aa3619bad28b577"
   },
   "source": [
    "<a id=\"2.2\"></a> <br>\n",
    "### 2.2 Define the type of each variable\n",
    "In practice, among the numerical variables, many corresponds to identifiers. *In the current dataset, the truly numerical variables are in fact rare*. Below, I make a list of the variables which are truly numerical, according the the description of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1485ef0a5822d9c0064e39f6b94977d336dd3698"
   },
   "outputs": [],
   "source": [
    "true_numerical_columns = [\n",
    "    'Census_ProcessorCoreCount',\n",
    "    'Census_PrimaryDiskTotalCapacity',\n",
    "    'Census_SystemVolumeTotalCapacity',\n",
    "    'Census_TotalPhysicalRAM',\n",
    "    'Census_InternalPrimaryDiagonalDisplaySizeInInches',\n",
    "    'Census_InternalPrimaryDisplayResolutionHorizontal',\n",
    "    'Census_InternalPrimaryDisplayResolutionVertical',\n",
    "    'Census_InternalBatteryNumberOfCharges'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "837cc82ceb02e7dcb519542f7bead3299c1e0761"
   },
   "source": [
    "We also list binary variables, since they can be treated as numericals by tree methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8bcb5afe9e9f6501f0f4f509d7c140617b259f69"
   },
   "outputs": [],
   "source": [
    "binary_variables = [c for c in train.columns if train[c].nunique() == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "746df3e5496acba451e89a05f75042e7ef5ddcd8"
   },
   "source": [
    "to finally make a census of the categorical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7f249c89e5deee0e36a0983ec6c2ab934ee13a8c"
   },
   "outputs": [],
   "source": [
    "categorical_columns = [c for c in train.columns \n",
    "                       if (c not in true_numerical_columns) & (c not in binary_variables)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "02a03355068b80bd404dc6c34f95ded2b138e344"
   },
   "outputs": [],
   "source": [
    "variables = {\n",
    "    'categorical_columns': len(categorical_columns),\n",
    "    'binary_variables': len(binary_variables),\n",
    "    'true_numerical_columns': len(true_numerical_columns)\n",
    "}\n",
    "pie_trace = go.Pie(labels=list(variables.keys()), values=list(variables.values()))\n",
    "layout = dict(title= \"Variable types\", height=400, width=800)\n",
    "fig = dict(data=[pie_trace], layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ad51d7daad193e0dab467f82dfad4c7ce7876d56"
   },
   "source": [
    "Most of the current variables are categories and we need to choose a method to treat them. **Depending on the cardinality of each variable**, we can opt for** one-hot-encoding, frequency or target encoding**. In the particular case of Light-GBM, we can also use the **built-in LGBM treatment of categoricals**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "bf2a4a2490650a68ca65991e08a0f6975a549102"
   },
   "outputs": [],
   "source": [
    "def card():\n",
    "    cardinality = []\n",
    "    for c in categorical_columns:\n",
    "        if c == 'MachineIdentifier': continue\n",
    "        cardinality.append([c, train[c].nunique()])\n",
    "    cardinality.sort(key = lambda x:x[1], reverse=False)\n",
    "\n",
    "    trace = go.Bar(y=[x[0] for x in cardinality],\n",
    "                   x=[x[1] for x in cardinality],\n",
    "                   orientation='h', marker=dict(color='rgb(49,130,189)'), name='train')\n",
    "\n",
    "    layout = go.Layout(\n",
    "        title='Categorical cardinality', height=1600, width=800,\n",
    "        xaxis=dict(\n",
    "            title='Number of categories',\n",
    "            titlefont=dict(size=16, color='rgb(107, 107, 107)'),\n",
    "            domain=[0.25, 1]\n",
    "        ),\n",
    "        barmode='group',\n",
    "        bargap=0.1,\n",
    "        bargroupgap=0.1\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=[trace], layout=layout)\n",
    "    iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "card()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fec10635dc57ca66edf9600bed8a4eca5958f780"
   },
   "source": [
    "<a id=\"2\"></a> <br>\n",
    "## 3. Feature Engineering\n",
    "<a id=\"3.1\"></a> <br>\n",
    "### 3.1 Frequency encoding\n",
    "For variables with large cardinality, an efficient encoding consists in ranking the categories with respect to their frequencies. These variables are then treated as numerical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "d8cc8d69a682a1a753c82ee69b6a3fc26c99ec3a"
   },
   "outputs": [],
   "source": [
    "def frequency_encoding(variable):\n",
    "    t = pd.concat([train[variable], test[variable]]).value_counts().reset_index()\n",
    "    t = t.reset_index()\n",
    "    t.loc[t[variable] == 1, 'level_0'] = np.nan\n",
    "    t.set_index('index', inplace=True)\n",
    "    max_label = t['level_0'].max() + 1\n",
    "    t.fillna(max_label, inplace=True)\n",
    "    return t.to_dict()['level_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5d59541ddae08fc488b48b36d426ab9ec43ebd60"
   },
   "outputs": [],
   "source": [
    "frequency_encoded_variables = [\n",
    "    'Census_OEMModelIdentifier',\n",
    "    'CityIdentifier',\n",
    "    'Census_FirmwareVersionIdentifier',\n",
    "    'AvSigVersion',\n",
    "    'Census_ProcessorModelIdentifier',\n",
    "    'Census_OEMNameIdentifier',\n",
    "    'DefaultBrowsersIdentifier'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8c18a1d420eeb72f8dfac35e2f9ce7bde823f66b"
   },
   "outputs": [],
   "source": [
    "for variable in tqdm(frequency_encoded_variables):\n",
    "    freq_enc_dict = frequency_encoding(variable)\n",
    "    train[variable] = train[variable].map(lambda x: freq_enc_dict.get(x, np.nan))\n",
    "    test[variable] = test[variable].map(lambda x: freq_enc_dict.get(x, np.nan))\n",
    "    categorical_columns.remove(variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "card()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try reducing the cardinality further by defining following function and calling it for the feature which we want to reduce the cardinality for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REDUCE CATEGORY CARDINALITY\n",
    "def relax_data(df_train, df_test, col):\n",
    "    cv1 = pd.DataFrame(df_train[col].value_counts().reset_index().rename({col:'train'},axis=1))\n",
    "    cv2 = pd.DataFrame(df_test[col].value_counts().reset_index().rename({col:'test'},axis=1))\n",
    "    cv3 = pd.merge(cv1,cv2,on='index',how='outer')\n",
    "    factor = len(df_test)/len(df_train)\n",
    "    cv3['train'].fillna(0,inplace=True)\n",
    "    cv3['test'].fillna(0,inplace=True)\n",
    "    cv3['remove'] = False\n",
    "    cv3['remove'] = cv3['remove'] | (cv3['train'] < len(df_train)/10000)\n",
    "    cv3['remove'] = cv3['remove'] | (factor*cv3['train'] < cv3['test']/3)\n",
    "    cv3['remove'] = cv3['remove'] | (factor*cv3['train'] > 3*cv3['test'])\n",
    "    cv3['new'] = cv3.apply(lambda x: x['index'] if x['remove']==False else 0,axis=1)\n",
    "    cv3['new'],_ = cv3['new'].factorize(sort=True)\n",
    "    cv3.set_index('index',inplace=True)\n",
    "    cc = cv3['new'].to_dict()\n",
    "    df_train[col] = df_train[col].map(cc)\n",
    "    #reduce_memory(df_train,col)\n",
    "    df_test[col] = df_test[col].map(cc)\n",
    "    #reduce_memory(df_test,col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relax_data(train,test,'AVProductStatesIdentifier')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot cardinality once again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "card()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.2\"></a> <br>\n",
    "### 3.1 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AppVersion2 indicates whether your Windows Defender is up to date. This is the second number from AppVersion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloaded timestamps from Microsoft and created this python dictionary that maps Microsoft's AvSigVersion (str) to a timestamp (datetime.datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AS timestamp\n",
    "datedictAS = np.load('../input/avsigversiontimestamps/AvSigVersionTimestamps.npy')[()]\n",
    "train['DateAS'] =train['AvSigVersion'].map(datedictAS)\n",
    "test['DateAS'] =test['AvSigVersion'].map(datedictAS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Engineered feature 'AppVersion2'\n",
    "train['AppVersion2'] = train['AppVersion'].map(lambda x: np.int(x.split('.')[1]))\n",
    "test['AppVersion2'] = test['AppVersion'].map(lambda x: np.int(x.split('.')[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lag1 is the difference between AvSigVersion_Date and Census_OSVersion_Date. Since AvSigVersion is the virus definitions for Windows Defender, this variable indicates whether Windows Defender is out-of-date by comparing it's last install with the date of the operating system. Out-of-date antivirus indicates that a user either has better antivirus or they don't use their computer often. In either case, they have less HasDetections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OS timestamp\n",
    "datedictOS = np.load('../input/osversiontimestamps/OSVersionTimestamps.npy')[()]\n",
    "train['DateOS'] = train['Census_OSVersion'].map(datedictOS)\n",
    "test['DateOS'] =test['Census_OSVersion'].map(datedictOS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Lag1'] = train['DateAS'] - train['DateOS']\n",
    "train['Lag1'] = train['Lag1'].map(lambda x: x.days//7)\n",
    "test['Lag1'] = test['DateAS'] - test['DateOS']\n",
    "test['Lag1'] = test['Lag1'].map(lambda x: x.days//7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "driveA is the ratio of harddrive partition used for the operating system with the total hard drive. Savy users install multiple operating systems and have a lower ratio. Savy users have reduced HasDetections.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENGINEERED FEATURE #4\n",
    "train['driveA'] = train['Census_SystemVolumeTotalCapacity'].astype('float')/train['Census_PrimaryDiskTotalCapacity'].astype('float')\n",
    "test['driveA'] = test['Census_SystemVolumeTotalCapacity'].astype('float')/test['Census_PrimaryDiskTotalCapacity'].astype('float')\n",
    "train['driveA'] = train['driveA'].astype('float32') \n",
    "test['driveA'] = test['driveA'].astype('float32') \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "driveB is the difference between harddrive partition used for the operating system and total hard drive. Responsible users manager their hard drives well. Responsible users have reduced HasDetections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENGINNERED FEATURE #5\n",
    "train['driveB'] = train['Census_PrimaryDiskTotalCapacity'].astype('float') - train['Census_SystemVolumeTotalCapacity'].astype('float')\n",
    "test['driveB'] = test['Census_PrimaryDiskTotalCapacity'].astype('float') - test['Census_SystemVolumeTotalCapacity'].astype('float')\n",
    "train['driveB'] = train['driveB'].astype('float32') \n",
    "test['driveB'] = test['driveB'].astype('float32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols6=['Lag1']\n",
    "cols8=['Lag5','driveB','driveA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "del train['DateAS'], train['DateOS'] #, train['DateBL']\n",
    "del test['DateAS'], test['DateOS'] #, test['DateBL']\n",
    "del datedictAS, datedictOS\n",
    "x=gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a correlation between the frequency of time variables (EngineVersion, AvSigVersion, AppVersion, Census_OSVersion, Census_OSBuildRevision) and HasDetections.\n",
    "This occurs for two reasons explained here. One reason is a leak and will lead to overfitting train and public test because I believe that private test is a random sample and therefore does not benefit from the leak. The second reason is because out-of-date antivirus produces a front tail with both reduced frequency and reduced HasDetections. To take advantage of these two correlations, you need to frequency encode the time variables separately for train and test.We have already done that for AvSigVersion.We will do it for the rest now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FE = ['EngineVersion',\n",
    "    'AppVersion',\n",
    "    'Census_OSVersion',\n",
    "    'Census_OSBuildRevision']\n",
    "\n",
    "for variable in tqdm(FE):\n",
    "    freq_enc_dict = frequency_encoding(variable)\n",
    "    train[variable] = train[variable].map(lambda x: freq_enc_dict.get(x, np.nan))\n",
    "    test[variable] = test[variable].map(lambda x: freq_enc_dict.get(x, np.nan))\n",
    "    categorical_columns.remove(variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After every variable was changed to dtype integer, each variable was tested one by one to see if making it categorical increases LGBM validation score. It was determined that the following 6 variables increased validation score. By making them categorical you prioritize them. So presumably these are important variables. Notice that they cover the variety of flavors of features. We have two geographical variables, one hardware variable, two software/virus variables, and one miscellenous. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every variable was removed one at a time and validation score was recorded. Validation verified that we can remove the following 18 variables without decreasing model accuracy. MachineIdentifier, ProductName, IsBeta, IsSxsPassiveMode, HasTpm, AutoSampleOptIn, PuaMode, UacLuaenable, Census_DeviceFamily, Census_ProcessorClass, Census_IsPortableOperatingSystem, Census_IsFlightsDisabled, Census_IsVirtualDevice, Census_OSSkuName, OsVer, Census_OSArchitecture, Census_OSInstallLanguageIdentifier, SMode. The first one is obvious. The next 12 have more than 98% of their data in one category value. The next 4 correlate with another variable greater than r=0.99. And the last variable SMode changes is distribution radically over time and leads to decreased CV and LB. The other variables don't decrease validation per say but they don't increase it either, so we can try running with or without removing them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a1355051b9350acb3f0bd3cc14f52f47082e0756"
   },
   "source": [
    "<a id=\"3.2\"></a> <br>\n",
    "### 3.2 Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8eec30f337f6893ed6ae0cdb348595e0b701eef2"
   },
   "outputs": [],
   "source": [
    "indexer = {}\n",
    "for col in tqdm(categorical_columns):\n",
    "    if col == 'MachineIdentifier': continue\n",
    "    _, indexer[col] = pd.factorize(train[col])\n",
    "    \n",
    "for col in tqdm(categorical_columns):\n",
    "    if col == 'MachineIdentifier': continue\n",
    "    train[col] = indexer[col].get_indexer(train[col])\n",
    "    test[col] = indexer[col].get_indexer(test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e30b4a83f31c0ee16c5d628438bb3dc2d123ca8e"
   },
   "outputs": [],
   "source": [
    "train = reduce_mem_usage(train)\n",
    "test = reduce_mem_usage(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b24463c6a0bf3603b5cff08702910f82dece28f3"
   },
   "outputs": [],
   "source": [
    "target = train['HasDetections']\n",
    "del train['HasDetections']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b91db8b5c98da4f014f1863ba7de18e241f517c6"
   },
   "source": [
    "<a id=\"3\"></a> <br>\n",
    "## 3. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "56d006d366fc5c2686249887b5d1f302d4a708f5"
   },
   "outputs": [],
   "source": [
    "param = {'num_leaves': 60,\n",
    "         'min_data_in_leaf': 60, \n",
    "         'objective':'binary',\n",
    "         'max_depth': -1,\n",
    "         'learning_rate': 0.1,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.8,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.8 ,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'auc',\n",
    "         \"lambda_l1\": 0.1,\n",
    "         \"random_state\": 133,\n",
    "         \"verbosity\": -1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7ab8ef627687c7602b81a9ac83f90ff3a2094d0c"
   },
   "source": [
    "We set the max number of iteration over folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9f306e20ae748715da17a2f15702ea1aa4d81497"
   },
   "outputs": [],
   "source": [
    "max_iter = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f2f01a6006dcb34ffb53ca33a055e592ee9e75e1"
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0e9f0fd37207aeaa33f3d758ed22a32b462fc93d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folds = KFold(n_splits=5, shuffle=True, random_state=15)\n",
    "oof = np.zeros(len(train))\n",
    "categorical_columns = [c for c in categorical_columns if c not in ['MachineIdentifier']]\n",
    "features = [c for c in train.columns if c not in ['MachineIdentifier']]\n",
    "predictions = np.zeros(len(test))\n",
    "start = time.time()\n",
    "feature_importance_df = pd.DataFrame()\n",
    "start_time= time.time()\n",
    "score = [0 for _ in range(folds.n_splits)]\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n",
    "    print(\"fold nÂ°{}\".format(fold_))\n",
    "    trn_data = lgb.Dataset(train.iloc[trn_idx][features],\n",
    "                           label=target.iloc[trn_idx],\n",
    "                           categorical_feature = categorical_columns\n",
    "                          )\n",
    "    val_data = lgb.Dataset(train.iloc[val_idx][features],\n",
    "                           label=target.iloc[val_idx],\n",
    "                           categorical_feature = categorical_columns\n",
    "                          )\n",
    "\n",
    "    num_round = 10000\n",
    "    clf = lgb.train(param,\n",
    "                    trn_data,\n",
    "                    num_round,\n",
    "                    valid_sets = [trn_data, val_data],\n",
    "                    verbose_eval=100,\n",
    "                    early_stopping_rounds = 200)\n",
    "    \n",
    "    oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = features\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance(importance_type='gain')\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "\n",
    "    # we perform predictions by chunks\n",
    "    initial_idx = 0\n",
    "    chunk_size = 1000000\n",
    "    current_pred = np.zeros(len(test))\n",
    "    while initial_idx < test.shape[0]:\n",
    "        final_idx = min(initial_idx + chunk_size, test.shape[0])\n",
    "        idx = range(initial_idx, final_idx)\n",
    "        current_pred[idx] = clf.predict(test.iloc[idx][features], num_iteration=clf.best_iteration)\n",
    "        initial_idx = final_idx\n",
    "    predictions += current_pred / min(folds.n_splits, max_iter)\n",
    "   \n",
    "    print(\"time elapsed: {:<5.2}s\".format((time.time() - start_time) / 3600))\n",
    "    score[fold_] = metrics.roc_auc_score(target.iloc[val_idx], oof[val_idx])\n",
    "    if fold_ == max_iter - 1: break\n",
    "        \n",
    "if (folds.n_splits == max_iter):\n",
    "    print(\"CV score: {:<8.5f}\".format(metrics.roc_auc_score(target, oof)))\n",
    "else:\n",
    "     print(\"CV score: {:<8.5f}\".format(sum(score) / max_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " predictions[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert into binary values\n",
    "y_pred=[]\n",
    "for i in range(len(test)):\n",
    "    if predictions[i]>=.5:       # setting threshold to .5\n",
    "       y_pred.append(1)\n",
    "    else:  \n",
    "       y_pred.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "15075259e27c5180c8e650ac9c9bc06f8ca2239b"
   },
   "source": [
    "<a id=\"4\"></a> <br>\n",
    "## 4. Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8b814f54b6ee5665fcadaa4b1a1bd28282fc2390",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cols = (feature_importance_df[[\"feature\", \"importance\"]]\n",
    "        .groupby(\"feature\")\n",
    "        .mean()\n",
    "        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n",
    "\n",
    "best_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n",
    "\n",
    "plt.figure(figsize=(14,25))\n",
    "sns.barplot(x=\"importance\",\n",
    "            y=\"feature\",\n",
    "            data=best_features.sort_values(by=\"importance\",\n",
    "                                           ascending=False))\n",
    "plt.title('LightGBM Features (avg over folds)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('lgbm_importances.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "08a4c881ae24d784e9ee3c197d1188e26fdb40c7"
   },
   "source": [
    "<a id=\"5\"></a> <br>\n",
    "## 5. Getting the predictions to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1fe8a68970387c8e57d373b4d0944731a32ccd51"
   },
   "outputs": [],
   "source": [
    "sub_df = pd.DataFrame({\"MachineIdentifier\": test[\"MachineIdentifier\"].values})\n",
    "sub_df[\"HasDetections\"] = y_pred\n",
    "sub_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "65cd74398d3f80186d65a2ff3431403a6846e229"
   },
   "outputs": [],
   "source": [
    "sub_df.to_csv(\"Predictions.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
